{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Component Analysis\n",
    "\n",
    "In this exercise, you will implement an ICA algorithm similar to the FastICA method described in the paper *\"A. Hyv√§rinen and E. Oja. 2000. Independent component analysis: algorithms and applications\"* linked from ISIS, and apply it to model the independent components of a distribution of image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.feature_extraction.image\n",
    "import utils\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we take a sample image, extract a collection of $(8 \\times 8)$ patches from it and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-95056e8e9fb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_sample_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'china.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_patches_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_patches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowimage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowpatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'utils' is not defined"
     ]
    }
   ],
   "source": [
    "I = sklearn.datasets.load_sample_image('china.jpg')\n",
    "X = sklearn.feature_extraction.image.extract_patches_2d(I, (8,8), max_patches=10000, random_state=0)\n",
    "utils.showimage(I)\n",
    "utils.showpatches(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starting point, the patches we have extracted are flattened to appear as abstract input vectors of $8 \\times 8 \\times 3 = 192$ dimensions. The input data is then centered and standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(len(X),-1)\n",
    "X = X - X.mean(axis=0)\n",
    "X = X / X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening (10 P)\n",
    "\n",
    "A precondition for applying the ICA procedure is that the input data has variance $1$ under any projection. This can be achieved by whitening, which is a transformation $\\mathcal{W}:\\mathbb{R}^d \\to \\mathbb{R}^d$ with $z = \\mathcal{W}(x)$ such that $\\mathrm{E}[zz^\\top] = I$.\n",
    "\n",
    "A simple procedure for whitening a collection of data points $x_1,\\dots,x_N$ (assumed to be centered) first computes the PCA components $u_1,\\dots,u_d$ of the data and then applies the following three consecutive steps:\n",
    "\n",
    " 1. project the data on the PCA components i.e. $p_{n,i} = x_n^\\top u_i$.\n",
    " 2. divide the projected data by the standard deviation in PCA space, i.e. $\\tilde{p}_{n,i} = p_{n,i} / \\text{std}(p_{:,i})$\n",
    " 3. backproject to the input space $z_n = \\sum_i \\tilde{p}_{n,i} u_i$.\n",
    "\n",
    "**Task:**\n",
    "\n",
    " * **Implement this whitening procedure, in particular, write a function that receives the input data matrix and returns the matrix containing all whitened data points.**\n",
    " \n",
    "For efficiency, your whitening procedure should be implemented in matrix form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitening(X):\n",
    "    Cov=X.T@X\n",
    "    w,v=np.linalg.eig(Cov)\n",
    "    idx=np.argsort(w)\n",
    "    w=w[idx]\n",
    "    v=v[:,idx]\n",
    "    p=X@v\n",
    "    assert p.shape[0]==X.shape[0]\n",
    "    assert p.shape[1]==X.shape[1]\n",
    "    p_new=p/p.std(axis=0)\n",
    "    Z=p_new@v.T\n",
    "    assert Z.shape[0]==X.shape[0]\n",
    "    assert Z.shape[1]==X.shape[1]\n",
    "    return Z\n",
    "Z=whitening(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below verifies graphically that whitening has removed correlations between the different input dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "f = plt.figure(figsize=(8,4))\n",
    "p = f.add_subplot(1,2,1); p.set_title('before whitening')\n",
    "p.imshow(numpy.dot(X.T,X)/len(X))\n",
    "p = f.add_subplot(1,2,2); p.set_title('after whitening')\n",
    "p.imshow(numpy.dot(Z.T,Z)/len(Z))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to get visual picture of what will enter into our ICA algorithm, the whitened data can be visualized in the same way as the original input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.showpatches(X)\n",
    "utils.showpatches(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that all high constrasts and spatial correlations have been removed after whitening. Remaining patterns include high-frequency textures and oriented edges of different colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ICA (20 P)\n",
    "\n",
    "\n",
    "We now would like to learn $h=64$ independent components of the distribution of whitened image patches. For this, we adopt a procedure similar to the FastICA procedure described in the paper above. In particular, we start with random weights $w_1,\\dots,w_h \\in \\mathbb{R}^d$ and iterate multiple times the sequence of operations:\n",
    "\n",
    " 1. $\\forall_{i=1}^d~w_i = \\mathbb{E}[x \\cdot g(w_i^\\top x)] - w_i \\cdot \\mathbb{E}[ g'(w_i^\\top x)]$\n",
    " 2. $w_1,\\dots,w_h=\\text{decorrelate}\\{w_1,\\dots,w_h\\}$\n",
    "\n",
    "where $\\mathbb{E}[\\cdot]$ denotes the expectation with the data distribution.\n",
    "\n",
    "The first step increases non-Gaussianity of the projected data. Here, we will make use of the nonquadratic function $G(x) = \\frac1a \\log \\cosh (a x)$ with $a=1.5$. This function admits as a derivative the function $g(x) = \\tanh(a x)$, and as a double derivative the function $g'(x) = a \\cdot (1-\\tanh^2(a x))$.\n",
    "\n",
    "The second step enforces that the learned projections are decorrelated, i.e.\\ $w_i^\\top w_j = 1_{i=j}$. It will be implemented by calling in an appropriate manner the whitening procedure which we have already implemented to decorrelate the different input dimensions.\n",
    "\n",
    "This procedure minimizes the non-Gaussianity of the projected data as measured by the objective:\n",
    "\n",
    "$$\n",
    "J(w) = \\sum_{i=1}^h (\\mathbb{E}[G(w_i^\\top x)] - \\mathbb{E}[G(\\varepsilon)])^2 \\qquad \\text{where} \\quad \\varepsilon \\sim \\mathcal{N}(0,1).\n",
    "$$\n",
    "\n",
    "\n",
    "**Task:**\n",
    "\n",
    "* **Implement the ICA procedure described above, run it for 200 iterations, and print the value of the objective function every 25 iterations.**\n",
    "\n",
    "In order to keep the learning procedure computationally affordable, the code must be parallelized, in particular, make use of numpy matrix multiplications instead of loops whenever it is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastICA(X):\n",
    "    def G(x):\n",
    "        return 1/1.5 * np.log(np.cosh(1.5 * x))\n",
    "    def g(x):\n",
    "        return np.tanh(1.5 * x)\n",
    "    def gprime(x):\n",
    "        return 1.5 * (1 - (np.dot(np.tanh(1.5 * x), np.tanh(1.5 * x))))\n",
    "    def J(y):\n",
    "        v = np.random.normal(size=y.shape)\n",
    "        return np.sum((np.mean(G(y),axis=1) - np.mean(G(v),axis=1))**2)\n",
    "    INTERESTIGN_COMPONENT = 1\n",
    "    W = np.random.normal(0,1,size=(X.shape[0], X.shape[1]))\n",
    "    h=X.shape[0]\n",
    "    for i in range(200):\n",
    "        print(\"it = \" + str(i) + \"\\tJ(W) = \" + str(J(np.dot(W, X.T))))\n",
    "        Wtx = np.dot(W, X.T)\n",
    "        gWtx = g(Wtx)\n",
    "        g_Wtx = gprime(Wtx)\n",
    "        W=(np.dot(gWtx,X)/h) - g_Wtx.mean(axis=1).reshape((-1,1)) * W\n",
    "        W=whitening(W)\n",
    "                 \n",
    "    return W\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the learned ICA components are in a space of same dimensions as the input data, they can also be visualized as image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=fastICA(X)\n",
    "utils.showpatches(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that an interesting decomposition appears, composed of frequency filters, edges filters and localized texture filters. The decomposition further aligns on specific directions of the RGB space, specifically yellow/blue and red/cyan.\n",
    "\n",
    "To verify that strongly non-Gaussian components have been learned, we build a histogram of projections on the various ICA components and compare it to histograms for random projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "plt.figure(figsize=(7,2))\n",
    "for i in range(64):\n",
    "    plt.hist(numpy.dot(Z,W[i]),bins=numpy.linspace(-12.5,12.5,26),histtype='step',log=True)\n",
    "plt.title('ICA projections')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,2))\n",
    "for i in range(64):\n",
    "    R = numpy.random.mtrand.RandomState(i).normal(0,1,Z.shape[1])\n",
    "    plt.hist(numpy.dot(Z,R/(R**2).sum()**.5),bins=numpy.linspace(-12.5,12.5,26),histtype='step',log=True)\n",
    "plt.title('random projections')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the ICA projections have much heavier tails. This is a typical characteristic of independent components of a data distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
